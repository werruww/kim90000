{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOk7k6Hv8zyM35QWzPRnq4J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim90000/2-rag-Successful/blob/main/suc_2rag_pdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###############################################################"
      ],
      "metadata": {
        "id": "0CoBwjfGXlOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu\n",
        "!pip install bitsandbytes\n",
        "!huggingface-cli login\n",
        "!pip install sentence-transformers langchain langchain-community tqdm pypdf faiss-gpu\n",
        "\n"
      ],
      "metadata": {
        "id": "mq_F1ZyqXipi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال رائع"
      ],
      "metadata": {
        "id": "_Z-8rrB_WHB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# التأكد من استخدام CUDA إذا كان متوفراً\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "def get_gpu_memory():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    return 0\n",
        "\n",
        "class RAGSystem:\n",
        "    def __init__(self, model_path=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                 embedding_model=\"BAAI/bge-large-en-v1.5\"):\n",
        "        self.device = device\n",
        "        self.setup_model(model_path)\n",
        "        self.setup_embeddings(embedding_model)\n",
        "        self.vector_db = None\n",
        "        self.qa_chain = None\n",
        "\n",
        "    def setup_model(self, model_path):\n",
        "        print(\"تحميل النموذج...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "\n",
        "        gpu_memory = get_gpu_memory()\n",
        "        dtype = torch.float16 if gpu_memory < 16 else torch.bfloat16\n",
        "\n",
        "        model_kwargs = {\n",
        "            \"torch_dtype\": dtype,\n",
        "            \"device_map\": \"auto\",\n",
        "            \"trust_remote_code\": True,\n",
        "            \"load_in_8bit\": gpu_memory < 8,\n",
        "            \"max_memory\": {0: f\"{int(gpu_memory * 0.9)}GB\"},\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                **model_kwargs\n",
        "            )\n",
        "\n",
        "            pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                max_new_tokens=2048,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                device_map=\"auto\",\n",
        "                batch_size=1,\n",
        "                model_kwargs={\"torch_dtype\": dtype}\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"خطأ أثناء تحميل النموذج: {str(e)}\")\n",
        "            model_kwargs.update({\n",
        "                \"load_in_8bit\": True,\n",
        "                \"torch_dtype\": torch.float16,\n",
        "            })\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                **model_kwargs\n",
        "            )\n",
        "            pipe = pipeline(\n",
        "                \"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer=self.tokenizer,\n",
        "                max_new_tokens=1024,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                device_map=\"auto\",\n",
        "                batch_size=1\n",
        "            )\n",
        "\n",
        "        self.llm = HuggingFacePipeline(pipeline=pipe)\n",
        "        print(\"تم تحميل النموذج بنجاح\")\n",
        "\n",
        "    def setup_embeddings(self, model_name):\n",
        "        print(\"تحميل نموذج التضمين...\")\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=model_name,\n",
        "            model_kwargs={'device': self.device},\n",
        "            encode_kwargs={'device': self.device, 'batch_size': 32}\n",
        "        )\n",
        "        print(\"تم تحميل نموذج التضمين بنجاح\")\n",
        "\n",
        "    def load_documents(self, pdf_paths):\n",
        "        print(\"تحميل المستندات...\")\n",
        "        loaders = [PyPDFLoader(path) for path in pdf_paths]\n",
        "        pages = []\n",
        "        for loader in loaders:\n",
        "            pages.extend(loader.load())\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1024,\n",
        "            chunk_overlap=64\n",
        "        )\n",
        "        doc_splits = text_splitter.split_documents(pages)\n",
        "\n",
        "        print(f\"تم تقسيم المستندات إلى {len(doc_splits)} جزء\")\n",
        "        return doc_splits\n",
        "\n",
        "    def create_vector_db(self, pdf_paths):\n",
        "        splits = self.load_documents(pdf_paths)\n",
        "        print(\"إنشاء قاعدة البيانات المتجهة...\")\n",
        "        self.vector_db = FAISS.from_documents(splits, self.embeddings)\n",
        "        print(\"تم إنشاء قاعدة البيانات المتجهة بنجاح\")\n",
        "\n",
        "        memory = ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            output_key='answer',\n",
        "            return_messages=True\n",
        "        )\n",
        "\n",
        "        self.qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "            self.llm,\n",
        "            retriever=self.vector_db.as_retriever(),\n",
        "            memory=memory,\n",
        "            chain_type=\"stuff\",\n",
        "            return_source_documents=True,\n",
        "            verbose=False\n",
        "        )\n",
        "        print(\"تم إعداد نظام الأسئلة والأجوبة بنجاح\")\n",
        "\n",
        "    def save_vector_db(self, path=\"vector_db\"):\n",
        "        if self.vector_db:\n",
        "            self.vector_db.save_local(path)\n",
        "            print(f\"تم حفظ قاعدة البيانات المتجهة في {path}\")\n",
        "\n",
        "    def load_vector_db(self, path=\"vector_db\", allow_unsafe=False):\n",
        "        if os.path.exists(path):\n",
        "            print(\"تحميل قاعدة البيانات المتجهة...\")\n",
        "            try:\n",
        "                self.vector_db = FAISS.load_local(\n",
        "                    path,\n",
        "                    self.embeddings,\n",
        "                    allow_dangerous_deserialization=allow_unsafe\n",
        "                )\n",
        "                print(\"تم تحميل قاعدة البيانات المتجهة بنجاح\")\n",
        "                return True\n",
        "            except ValueError as e:\n",
        "                print(f\"تحذير: {str(e)}\")\n",
        "                print(\"\\nتأكد من أمان الملف أو استخدم allow_unsafe=True\")\n",
        "                return False\n",
        "        return False\n",
        "\n",
        "    def query(self, question, chat_history=[]):\n",
        "        if not self.qa_chain:\n",
        "            raise ValueError(\"يجب إنشاء قاعدة البيانات المتجهة أولاً\")\n",
        "\n",
        "        formatted_history = []\n",
        "        for user_msg, bot_msg in chat_history:\n",
        "            formatted_history.append(f\"User: {user_msg}\")\n",
        "            formatted_history.append(f\"Assistant: {bot_msg}\")\n",
        "\n",
        "        response = self.qa_chain.invoke({\n",
        "            \"question\": question,\n",
        "            \"chat_history\": formatted_history\n",
        "        })\n",
        "\n",
        "        answer = response[\"answer\"]\n",
        "        if \"Helpful Answer:\" in answer:\n",
        "            answer = answer.split(\"Helpful Answer:\")[-1].strip()\n",
        "\n",
        "        sources = [\n",
        "            {\n",
        "                \"content\": doc.page_content.strip(),\n",
        "                \"page\": doc.metadata[\"page\"] + 1\n",
        "            }\n",
        "            for doc in response[\"source_documents\"][:3]\n",
        "        ]\n",
        "\n",
        "        return answer, sources\n",
        "\n",
        "def main():\n",
        "    rag = RAGSystem()\n",
        "    pdf_files = [\"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"]  # ضع مسار ملفات PDF هنا\n",
        "    vector_db_path = \"vector_db\"\n",
        "\n",
        "    # التحقق من وجود قاعدة البيانات\n",
        "    if not rag.load_vector_db(vector_db_path, allow_unsafe=True):\n",
        "        rag.create_vector_db(pdf_files)\n",
        "        rag.save_vector_db(vector_db_path)\n",
        "\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nاكتب سؤالك (او 'خروج' للإنهاء): \")\n",
        "        if question.lower() in ['exit', 'quit', 'خروج']:\n",
        "            break\n",
        "\n",
        "        answer, sources = rag.query(question, chat_history)\n",
        "        chat_history.append((question, answer))\n",
        "\n",
        "        print(f\"\\nالإجابة: {answer}\")\n",
        "        print(\"\\nالمصادر:\")\n",
        "        for src in sources:\n",
        "            print(f\"المحتوى: {src['content']} - الصفحة: {src['page']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "8YtHL5rzVOiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####################################################################"
      ],
      "metadata": {
        "id": "6gUjO7PvXn9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "شغال رائع"
      ],
      "metadata": {
        "id": "hZEptHoTX2e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "!pip install --upgrade langchain transformers chromadb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "\n",
        "class SentenceTransformerEmbeddings(Embeddings):\n",
        "    \"\"\"Custom embedding class for SentenceTransformer models.\"\"\"\n",
        "    def __init__(self, model_name):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts, show_progress_bar=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode(text, show_progress_bar=False).tolist()\n",
        "\n",
        "def setup_chroma_db(pdf_path, embedding_model_name=\"hkunlp/instructor-large\"):\n",
        "    \"\"\"Load PDF, split into chunks, and set up ChromaDB with embeddings.\"\"\"\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load_and_split()\n",
        "    embeddings = SentenceTransformerEmbeddings(embedding_model_name)\n",
        "    db = Chroma.from_documents(documents, embeddings)\n",
        "    return db\n",
        "\n",
        "def setup_mistral_pipeline(model_name=\"mistralai/Mistral-7B-Instruct-v0.2\", device=\"cuda\"):\n",
        "    \"\"\"Set up the Mistral model as a HuggingFace pipeline.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "    pipeline_model = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=1024,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )\n",
        "    # Pass the pipeline object as a keyword argument to HuggingFacePipeline\n",
        "    return HuggingFacePipeline(pipeline=pipeline_model)\n",
        "\n",
        "def build_qa_chain(retriever, llm):\n",
        "    \"\"\"Build a QA chain with a retriever and language model.\"\"\"\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=\"\"\"\n",
        "        Use the following context to answer the question:\n",
        "        Context: {context}\n",
        "        Question: {question}\n",
        "        Answer in detail.\n",
        "        \"\"\"\n",
        "    )\n",
        "    qa_chain = RetrievalQA.from_chain_type(\n",
        "        retriever=retriever,\n",
        "        llm=llm,\n",
        "        chain_type=\"stuff\",\n",
        "        return_source_documents=True,\n",
        "        chain_type_kwargs={\"prompt\": prompt_template}\n",
        "    )\n",
        "    return qa_chain\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths and model names\n",
        "    pdf_path = \"/content/The_Lightning_Thief_-_Percy_Jackson_1-10.pdf\"  # Path to your PDF\n",
        "    embedding_model_name = \"hkunlp/instructor-large\"\n",
        "    mistral_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "    # Step 1: Set up ChromaDB\n",
        "    print(\"Setting up ChromaDB...\")\n",
        "    vector_db = setup_chroma_db(pdf_path, embedding_model_name)\n",
        "    retriever = vector_db.as_retriever()\n",
        "\n",
        "    # Step 2: Set up Mistral pipeline\n",
        "    print(\"Setting up Mistral pipeline...\")\n",
        "    mistral_pipeline = setup_mistral_pipeline(mistral_model_name)\n",
        "\n",
        "    # Step 3: Build QA Chain\n",
        "    print(\"Building QA chain...\")\n",
        "    qa_chain = build_qa_chain(retriever, mistral_pipeline)\n",
        "\n",
        "    # Step 4: Ask a question\n",
        "    question = \"What is the main topic of the document?\"\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    # Step 5: Get the answer\n",
        "    result = qa_chain({\"query\": question})\n",
        "    answer = result[\"result\"]\n",
        "    source_documents = result[\"source_documents\"]\n",
        "\n",
        "    # Step 6: Display the answer and sources\n",
        "    print(\"\\nAnswer:\")\n",
        "    print(answer)\n",
        "    print(\"\\nSource Documents:\")\n",
        "    for doc in source_documents:\n",
        "        print(doc.page_content)\n"
      ],
      "metadata": {
        "id": "XAwdlLTyX2e7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}